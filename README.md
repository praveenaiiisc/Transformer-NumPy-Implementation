# Transformer-NumPy-Implementation
*Implement a self-attention block from scratch. Make the token length and number of attention layers as the hyper-parameters.
*Solve the 10-class classification problem with use a transformer model with self attention on PCAâ€™ed features with images as inputs,with the metrics for the classification are accuracy and F1 score
*Here use TF-IDF as embeddings (https://en.wikipedia.org/wiki/Tf%E2%80%93idf. use SkLearn only for this. Define a 12-class classification problem using the top 12 categories. The input features are taken from headlines, Solve this 12-class classification problem using with a transformer with self-attention. Append all the data with zeros to convert them into the same size,with the metrics for the classification are accuracy and F1 score


